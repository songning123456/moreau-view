### 微服务


#### 你们的系统使用了哪种服务框架？为什么要这样技术选型？
| ———— | Spring Cloud | Dubbo | 
| :----- | :----- | :----- | 
| 并发性能 | 使用的是HTTP协议，性能与Dubbo对比稍微差点。 | 是一款优秀的RPC框架，并发能力比Spring Cloud强。 | 
| 注册中心 | 有全家桶配置中心: eureka nacos，亦可以选择Zookeeper。 | 一般选择Zookeeper。 | 
| 分布式配置中心 | nacos / Spring Cloud Config | Apollo | 
| 网关 | Zuul / Spring Cloud Gateway  | 需引入其他网关组件 | 
| 负载均衡 | ribbon | 自带负载均衡 | 
| 熔断功能 | hystrix | 需引入其他熔断框架 | 
| 社区活跃度 | 活跃，版本更新快 | 不活跃 | 


#### 如果让你设计一个RPC框架，该从哪些方向考虑？
| 要点 | 解释 | 
| :----- | :----- | 
| <div style="width: 150px">服务发现与服务注册</div> | 1.如果我们想在Service A中调用Service B，那么我们首先得知道Service B的地址。所以，我们需要有一个服务注册中心，通过这个中心，服务可以把自己的信息注册进来，也可以获取到别的服务的信息；<br> 2.客户端也需要watch服务注册中心的目标服务的地址的变化。 | 
| <div style="width: 150px">网络通信</div> | 1.服务和服务之间的网络通信模型， NIO/IO等等；<br> 2.客户端如何复用与服务端的连接， 而不是每次请求都重新创建一个新连接；<br> 3.客户端收到返回后，如何知道是哪个请求的返回并且做出正确处理。 | 
| <div style="width: 150px">消息的序列化</div> | 服务间通信的消息通过什么方式进行序列化？Hessian，XML、JSON、Protobuf…，甚至Java原生的序列化方式，你总得选择一个。 | 
| <div style="width: 150px">负载均衡</div> | 客户端通过服务注册中心拿到一堆地址，该调哪个呢？最简单的方式，可以通过RR、WRR的方式去做LB。 | 
| <div style="width: 150px">容灾</div> | 1.健康监测：在某一个服务节点挂掉的时候， 如何在服务注册中心删去这个服务地址？<br> 2.服务调用超时与重试：在调用一个服务实例的时候，如果超时或者报错，怎么处理？<br> 3.服务限流：如何限制最大并发数？这个又可以从客户端和服务端两个角度分析。<br> | 


#### 能画一张图说说Spring Cloud的核心架构吗？它又是如何调用的？
![SpringCloud](/images/MicroService/SpringCloud.jpg)


1. 首先每个服务启动的时候都需要往注册中心进行注册。
2. 用户先对网关发起下单请求，网关收到请求后发现呃，是下单操作，要到订单系统，然后把请求路由到订单系统。
3. 订单系统啪啦啪啦一顿操作，然后通过Feign去调用库存系统减库存，通知仓储服务发货，调用积分系统加积分。
4. 在发起调用之前，订单系统还得通过Ribbon去注册中心去拉取各系统的注册表信息，并且挑一台机器给Feign来发起网络调用。


#### 你们的服务注册中心进行过选型调研吗？对比一Eureka和Zookeeper？
Zookeeper基于CP，不保证高可用，如果Zookeeper正在选主，或者Zookeeper集群中半数以上机器不可用，那么将无法获得数据。Eureka基于AP，能保证高可用，即使所有机器都挂了，也能拿到本地缓存的数据。作为注册中心，其实配置是不经常变动的，只有发版和机器出故障时会变。对于不经常变动的配置来说，CP是不合适的，而AP在遇到问题时可以用牺牲一致性来保证可用性，既返回旧数据，缓存数据。所以理论上Eureka是更适合作注册中心。而现实环境中大部分项目可能会使用Zookeeper，那是因为集群不够大，并且基本不会遇到用做注册中心的机器一半以上都挂了的情况。所以实际上也没什么大问题。
    

#### 你们对网关的技术选型是怎么考虑的？能对比一下各种网关的优劣吗？
1. `动态路由`<br>
新开发某个服务，动态把请求路径和服务的映射关系热加载到网关里去；服务增减机器，网关自动热感知。
2. `灰度发布`
3. `授权认证`
4. `性能监控`<br>
每个API接口的耗时、成功率、QPS。
5. `系统日志`
6. `数据缓存`
7. `限流熔断`


| ———— | Zuul1.x | Spring Cloud Gateway |
| :----- | :----- | :----- | 
| <div style="width: 150px">实现</div> | 基于Servlet2.x构建，使用阻塞的API。| 基于Spring 5、Project Reactor、Spring Boot 2，使用非阻塞式的API。 |
| <div style="width: 150px">长连接</div> | 不支持 | 支持 |
| <div style="width: 150px">不适用场景</div> | 后端服务响应慢或者高并发场景下，因为线程数量是固定(有限)的，线程容易被耗尽，导致新请求被拒绝。 | 中小流量的项目，使用Zuul1.x更合适。 |
| <div style="width: 150px">限流</div> | 无 | 内置限流过滤器 |
| <div style="width: 150px">上手难度</div> | 同步编程，上手简单。| 门槛较高，上手难度中等。 |
| <div style="width: 150px">Spring Cloud集成</div> | 是 | 是 |
| <div style="width: 150px">Sentinel集成</div> | 是 | 是 |
| <div style="width: 150px">技术栈沉淀</div> | Zuul1开源近七年，经受考验，稳定成熟。| 未见实际落地案例。|
| <div style="width: 150px">Github used by</div> | 1007 repositories | 102 repositories |
| <div style="width: 150px">Github issues</div> | 88 Open / 2736 Closed | 135 Open / 850 Closed |
| <div style="width: 150px">官方性能对比</div> | 2.09k / 12.56ms | 3.24k / 6.61ms |


Zuul1的开源时间很早，Netflix、Riot、携程、拍拍贷等公司都已经在生产环境中使用，自身经受了实践考验，是生产级的API网关产品。Gateway在2019年离开Spring Cloud孵化器，应用于生产的案例少，稳定性有待考证。从性能方面比较，两种产品在流量小的场景下性能表现差不多；并发高的场景下Gateway性能要好很多。从开发方面比较，Zuul1编程模型简单，易于扩展；Gateway编程模型稍难，代码阅读难度要比Zuul高不少，扩展也稍复杂一些。

    
#### 如果系统访问量比现在增加10倍，你们考虑过系统的扩容方案吗？
| 方案 | 解释 | 
| :----- | :----- | 
| 网关 | 横向加机器 |
| 注册中心 | 纵向升配置 |
| 数据库 | 纵向升配置 |


网关直接多部署10倍的机器即可，前面的Nginx做会负载均衡，把流量均匀分发给各个网关机器。服务扩容，都很简单的，多加机器，部署启动，自动注册到注册中心去。此时其他服务会自动感知到你的服务多加了一些机器。服务实例变多了10倍，此时几十个服务实例，几百个服务实例，对eureka机器会造成每秒几百请求，没问题，eureka机器，8核16G的配置，单机抗上千请求，很轻松。数据库本来是每秒几百请求，10倍，每秒高峰期是三四千请求，横向扩容很麻烦，此时可以考虑给单个数据库部署的机器提高配置，32核128G高配物理机，每秒钟抗几千请求问题不大。


#### 唯一ID生成机制中的snowflake算法的时钟回拨问题如何解决？
![snowflake](/images/MicroService/snowflake.jpg)


首先, snowflake的末尾12位是序列号, 用来记录同一毫秒内产生的不同id, 同一毫秒总共可以产生4096个id, 每一毫秒的序列号都是从0这个基础序列号开始递增假设我们的业务系统在单机上的QPS为3w/s, 那么其实平均每毫秒只需要产生30个id即可, 远没有达到设计的4096, 也就是说通常情况下序列号的使用都是处在一个低水位, 当发生时钟回拨的时候, 这些尚未被使用的序号就可以派上用场了。因此, 可以对给定的基础序列号稍加修改, 后面每发生一次时钟回拨就将基础序列号加上指定的步长, 例如开始时是从0递增, 发生一次时钟回拨后从1024开始递增, 再发生一次时钟回拨则从2048递增, 这样还能够满足3次的时钟回拨到同一时间点(发生这种操作就有点扯了)。


#### 分布式的“有状态”和“无状态”？
| 无状态 | 有状态 |
| :----- | :----- | 
| 对单次请求的处理不依赖其他请求。也就是说，处理一次请求所需的全部信息，要么都包含在这个请求里，要么可以从外部获取到(比如说数据库)，服务器本身不存储任何信息。 | 它会在自身保存一些数据，先后的请求是有关联的。|


1. 单体条件下面，服务只有一个，因此状态每个时刻也就只有一种状态。在分布式集群环境下面，就存在一个状态同步问题，因此也有有状态服务设计和无状态服务设计。比如session，如果session保存在每一台服务器上，那么就是有状态设计，可能会出现集群内，服务状态不一致的现象；如果session由专门的一台服务器来保存，就是无状态设计，服务不保存状态，需要的时候从同一的服务器中获取，保证了服务在任何时刻的状态一致。
2. 有状态的服务，会有比较明显的缺点，服务间数据需要同步，成为副本关系，逻辑复杂也浪费资源 ；无状态的应用服务器，不保存上下文信息，只负责对用户的每次请求提交数据进行处理然后返回处理结果 无状态应用服务器之间是对等的关系，无依赖，请求到哪个服务器，处理结果都一样的。


#### Skywalking中的traceId是如何在服务之间传递的？
👉 [基于trace_id链路追踪](https://blog.csdn.net/icansoicrazy/article/details/108359565)


#### Apollo配置中心动态生效实现原理？
👉 [Apollo配置中心动态生效实现原理](https://blog.csdn.net/fedorafrog/article/details/103919805)


👉 [携程开源配置中心Apollo的设计与实现](https://zhuanlan.zhihu.com/p/28723169)


Apollo配置中心动态生效机制，是基于`http长轮询请求`和`Spring扩展机制`实现的。在Spring容器启动过程中，Apollo通过自定义的BeanPostProcessor和BeanFactoryPostProcessor将参数中包含${...}占位符和@Value注解的Bean注册到Apollo框架中定义的注册表中。然后通过http长轮询不断的去获取服务端的配置信息，一旦配置发生变化，Apollo会根据变化的配置key找到对应的Bean，然后修改Bean的属性，从而实现了配置动态生效的特性。
